# sys.path.extend([r'C:\Users\Stroman\PycharmProjects\pantheon\venv'])
# sys.path.extend([r'C:\Users\Stroman\PycharmProjects\pantheon\venv\test_functions'])

import numpy as np
import pysapm
import DCM_in_pantheon
import copy
import matplotlib.pyplot as plt
import matplotlib
import scipy.stats as sps
import DCM_in_pantheon as dcm
import os

import scipy

from sklearn.linear_model import LinearRegression
import sklearn
from sklearn.decomposition import PCA
import time

# for saving data as mat file and testing in matlab
from scipy.io import savemat

matplotlib.use('TkAgg')   # explicitly set this - it might help with displaying figures in different environments


def reduce_matrix_L1norm(Mconn, Nintrinsic):
	nr, nr1 = np.shape(Mconn)
	e, v = np.linalg.eig(Mconn)

	M = np.zeros((nbeta, nbeta), dtype=np.complex_)
	S = np.zeros((nbeta, nbeta), dtype=np.complex_)
	for nn in range(nbeta):
		M[nn, nn] = copy.deepcopy(e[nn])
	for nn in range(nbeta):
		S[:, nn] = copy.deepcopy(v[:, nn])
	Mconn2 = np.real(S @ M @ np.linalg.inv(S))
	L1 = np.sum(np.abs(Mconn2))

	escale = np.ones(nr)  # starting point
	last_escale = copy.deepcopy(escale)
	last_L1 = np.sum(np.abs(Mconn2))
	last_Mconn = copy.deepcopy(Mconn2)

	dv = 0.01
	alpha = 0.01
	nitermax = 200
	alphamin = 1e-5
	iter = 0

	while (alpha > alphamin) & (iter < nitermax):
		iter += 1
		update = 0
		print('iter: {}  nr: {}  N intrinisic {}'.format(iter, nr, Nintrinsic))

		e, v = np.linalg.eig(Mconn)
		M = np.zeros((nbeta, nbeta), dtype=np.complex_)
		S = np.zeros((nbeta, nbeta), dtype=np.complex_)
		for nn in range(nbeta):
			M[nn, nn] = escale[nn] * e[nn]
		for nn in range(nbeta):
			S[:, nn] = escale[nn] * v[:, nn]
			# S[:, nn] = 1.0 * v[:, nn]
		Mconn_sim = np.real(S @ M @ np.linalg.inv(S))
		L1_1 = np.sum(np.abs(Mconn_sim))

		for rr in range(nr-Nintrinsic):
			temp_escale = copy.deepcopy(escale)
			temp_escale[rr] += dv/2.0
			M = np.zeros((nbeta, nbeta), dtype=np.complex_)
			S = np.zeros((nbeta, nbeta), dtype=np.complex_)

			e, v = np.linalg.eig(last_Mconn)
			for nn in range(nbeta):
				M[nn, nn] = temp_escale[nn]*e[nn]
			for nn in range(nbeta):
				S[:, nn] = temp_escale[nn] * v[:, nn]
				# S[:, nn] = copy.deepcopy(v[:,nn])

			Mconn_sim = np.real(S @ M @ np.linalg.inv(S))
			ep, vp = np.linalg.eig(Mconn_sim)
			L1p = np.sum(np.abs(Mconn_sim))

			temp_escale = copy.deepcopy(escale)
			temp_escale[rr] -= dv/2.0
			M = np.zeros((nbeta, nbeta), dtype=np.complex_)
			S = np.zeros((nbeta, nbeta), dtype=np.complex_)

			e,v = np.linalg.eig(last_Mconn)
			for nn in range(nbeta):
				M[nn, nn] = temp_escale[nn]*e[nn]
			for nn in range(nbeta):
				S[:, nn] = temp_escale[nn] * v[:, nn]
				# S[:, nn] = copy.deepcopy(v[:,nn])

			Mconn_sim = np.real(S @ M @ np.linalg.inv(S))
			em, vm = np.linalg.eig(Mconn_sim)
			L1m = np.sum(np.abs(Mconn_sim))

			dL1_dv = (newL1-L1_1)/dv

			escale[rr] -= alpha*dL1_dv

			e, v = np.linalg.eig(last_Mconn)
			M = np.zeros((nbeta, nbeta), dtype=np.complex_)
			S = np.zeros((nbeta, nbeta), dtype=np.complex_)
			for nn in range(nbeta):
				M[nn, nn] = escale[nn] * e[nn]
			for nn in range(nbeta):
				S[:, nn] = escale[nn] * v[:, nn]
				# S[:, nn] = copy.deepcopy(v[:, nn])
			Mconn_sim = np.real(S @ M @ np.linalg.inv(S))
			L1 = np.sum(np.abs(Mconn_sim))

			if L1 < last_L1:
				L1 = copy.deepcopy(newL1)
				last_L1 = copy.deepcopy(newL1)
				last_escale = copy.deepcopy(escale)
				last_Mconn = copy.deepcopy(Mconn_sim)
				update += 1
				print('iter: {}   L1 = {:.4f}'.format(iter,last_L1))
			else:
				L1 = copy.deepcopy(last_L1)
				escale = copy.deepcopy(last_escale)
				print('iter: {}  no improvement  L1 = {:.4f}'.format(iter,last_L1))

		if update == 0:
			alpha *= 0.5

	return last_Mconn



def define_network_data_based_on_SAPM_method(networkfile):
	network, nclusterlist, sem_region_list, fintrinsic_count, vintrinsic_count, fintrinsic_base = pysapm.load_network_model_w_intrinsics(networkfile)
	Nintrinsic = fintrinsic_count + vintrinsic_count
	nbeta = len(nclusterlist)
	nregions = nbeta - Nintrinsic
	networktargetlist = [network[x]['target'] for x in range(len(network))]

	# setup for simulation
	beta_pair = []
	Mconn = np.zeros((nbeta, nbeta))
	Minput = np.zeros((nregions, nbeta))
	count = 0
	for nn in range(len(network)):
		target = network[nn]['targetnum']
		sources = network[nn]['sourcenums']
		for mm in range(len(sources)):
			source = sources[mm]
			count += 1
			beta_pair.append([target, source])
			Mconn[target, source] = count
			Minput[target, source] = 1
			if source >= nregions:  # intrinsic input
				Mconn[source, source] = 1  # set the intrinsic beta values

	# prep to index Mconn for updating beta values
	beta_pair = np.array(beta_pair)
	ctarget = beta_pair[:, 0]
	csource = beta_pair[:, 1]

	dtarget = copy.deepcopy(ctarget)
	dsource = copy.deepcopy(csource)

	beta_list = []
	targetnumlist = []
	beta_id = []
	sourcelist = []
	for nn in range(len(network)):
		target = network[nn]['targetnum']
		sources = network[nn]['sourcenums']
		targetnumlist += [target]
		for mm in range(len(sources)):
			source = sources[mm]
			sourcelist += [source]
			betaname = '{}_{}'.format(source, target)
			entry = {'name': betaname, 'number': nbeta, 'pair': [source, target]}
			beta_list.append(entry)
			beta_id += [1000 * source + target]

	latent_flag = np.zeros(len(ctarget))
	found_latent_list = []
	for nn in range(len(ctarget)):
		if csource[nn] >= nregions and ctarget[nn] < nregions:
			found_latent_list += [csource[nn]]
			occurence = np.count_nonzero(found_latent_list == csource[nn])
			latent_flag[nn] = csource[nn] - nregions + 1

	reciprocal_flag = np.zeros(len(ctarget))
	for nn in range(len(ctarget)):
		spair = beta_list[csource[nn]]['pair']
		tpair = beta_list[ctarget[nn]]['pair']
		if spair[0] == tpair[1]:
			reciprocal_flag[nn] = 1

	# assign random DB values
	betavals = 0.4*np.random.randn(len(ctarget))
	c = np.where(latent_flag > 0)[0]
	betavals[c] = 1.0
	Mconn[ctarget, csource] = betavals
	e, v = np.linalg.eig(Mconn)
	# v[:,-Nintrinsic:] = (np.real(v[:,-Nintrinsic:])).astype(complex)

	M = np.zeros((nbeta, nbeta), dtype=np.complex_)
	S = np.zeros((nbeta, nbeta), dtype=np.complex_)

	# escale = [0.1, 0.2, 0.3, 0.4, 1.0, 1.0]
	# escale = np.ones(nbeta)
	for nn in range(nbeta):
		M[nn, nn] = copy.deepcopy(e[nn])

	for nn in range(nbeta):
		# normval = np.sqrt(v[:,nn].T @ v[:,nn])
		S[:, nn] = copy.deepcopy(v[:, nn])

	# checks
	Mconn_sim = np.real(S @ M @ np.linalg.inv(S))
	es, vs = np.linalg.eig(Mconn_sim)

	# need to make Mconn with same eigenvectors but with lowest L1 norm


	if Nintrinsic >= 1:
		L1 = np.real(v[:, nregions]) / np.real(v[nregions, nregions])
	if Nintrinsic >= 2:
		L2 = np.real(v[:, nregions + 1]) / np.real(v[nregions + 1, nregions + 1])
	if Nintrinsic >= 3:
		L3 = np.real(v[:, nregions + 2]) / np.real(v[nregions + 2, nregions + 2])
	if Nintrinsic >= 4:
		L4 = np.real(v[:, nregions + 3]) / np.real(v[nregions + 3, nregions + 3])

	# theoretically, now the values in Mconn can describe a consistent network
	# pick time-courses for latent inputs
	tsize = 40

	signal = 0.02

	nblocks = 3
	block_length = 5
	magnitude = signal
	p1 = generate_block_paradigm(magnitude, tsize, nblocks, block_length)

	nblocks = 7
	block_length = 3
	magnitude = 1.5 * signal
	p2 = generate_block_paradigm(magnitude, tsize, nblocks, block_length)

	nblocks = 1
	block_length = 12
	magnitude = 0.5 * signal
	p3 = generate_block_paradigm(magnitude, tsize, nblocks, block_length)

	# Sout = Mconn @ Sout :   but, this approach uses the SAPM method ... problem?
	Sout = L1[:, np.newaxis] @ (p1[:, np.newaxis]).T
	if Nintrinsic >= 2:
		Sout += L2[:, np.newaxis] @ (p2[:, np.newaxis]).T
	if Nintrinsic >= 3:
		Sout += L3[:, np.newaxis] @ (p3[:, np.newaxis]).T
	if Nintrinsic >= 4:
		Sout += L4[:, np.newaxis] @ (p4[:, np.newaxis]).T

	nruns_per_person = 5

	TR = 4.0
	hrf = HRF(TR)  # get ready with the hemodynamic response function

	Sout_BOLD = np.zeros((nbeta, tsize * nruns_per_person))
	for rr in range(nbeta):
		temp = np.convolve(Sout[rr, :], hrf, mode='full')[:tsize]
		Sout_BOLD[rr, :] = np.tile(temp, nruns_per_person)

	Sin_BOLD = Minput @ Sout_BOLD

	noise = 0.001 * signal * np.random.randn(nregions, tsize * nruns_per_person)  # add some random noise
	Sin_BOLD += noise

	Sout_neural = np.zeros((nbeta, tsize * nruns_per_person))
	for rr in range(nbeta):
		Sout_neural[rr, :] = np.tile(Sout[rr, :], nruns_per_person)

	Mintrinsic = copy.deepcopy(Sout_neural[-Nintrinsic:, :])
	Mintrinsic_BOLD = copy.deepcopy(Sout_BOLD[-Nintrinsic:, :])

	Sin_neural = Minput @ Sout_neural
	Sin_neural += noise

	tsize_full = tsize * nruns_per_person

	return Sin_BOLD, Sin_neural, Sout_BOLD, Sout_neural, Mintrinsic, Mintrinsic_BOLD, \
		   network, sem_region_list, beta_list, fintrinsic_count, vintrinsic_count, hrf, \
		   csource, ctarget, dsource, dtarget, nruns_per_person, tsize_full, Mconn, Minput, \
		   latent_flag, reciprocal_flag,  fintrinsic_count, vintrinsic_count


def define_network_data_based_on_DCM_method(networkfile):
	network, nclusterlist, sem_region_list, fintrinsic_count, vintrinsic_count, fintrinsic_base = pysapm.load_network_model_w_intrinsics(networkfile)
	Nintrinsic = fintrinsic_count + vintrinsic_count
	nbeta = len(nclusterlist)
	nregions = nbeta - Nintrinsic
	networktargetlist = [network[x]['target'] for x in range(len(network))]

	TE = 0.030
	TR = 4.0

	# setup for simulation
	beta_pair = []
	Mconn = np.zeros((nbeta, nbeta))
	Minput = np.zeros((nregions, nbeta))
	count = 0
	for nn in range(len(network)):
		target = network[nn]['targetnum']
		sources = network[nn]['sourcenums']
		for mm in range(len(sources)):
			source = sources[mm]
			count += 1
			beta_pair.append([target, source])
			Mconn[target, source] = count
			Minput[target, source] = 1
			if source >= nregions:  # intrinsic input
				Mconn[source, source] = 1  # set the intrinsic beta values

	# prep to index Mconn for updating beta values
	beta_pair = np.array(beta_pair)
	ctarget = beta_pair[:, 0]
	csource = beta_pair[:, 1]

	dtarget = copy.deepcopy(ctarget)
	dsource = copy.deepcopy(csource)

	ctarget2 = []
	csource2 = []
	for nn in range(len(ctarget)):
		if csource[nn] < nregions:
			ctarget2 += [ctarget[nn]]
			csource2 += [csource[nn]]

	beta_list = []
	targetnumlist = []
	beta_id = []
	sourcelist = []
	for nn in range(len(network)):
		target = network[nn]['targetnum']
		sources = network[nn]['sourcenums']
		targetnumlist += [target]
		for mm in range(len(sources)):
			source = sources[mm]
			sourcelist += [source]
			betaname = '{}_{}'.format(source, target)
			entry = {'name': betaname, 'number': nbeta, 'pair': [source, target]}
			beta_list.append(entry)
			beta_id += [1000 * source + target]

	latent_flag = np.zeros(len(ctarget))
	found_latent_list = []
	for nn in range(len(ctarget)):
		if csource[nn] >= nregions and ctarget[nn] < nregions:
			found_latent_list += [csource[nn]]
			occurence = np.count_nonzero(found_latent_list == csource[nn])
			latent_flag[nn] = csource[nn] - nregions + 1

	reciprocal_flag = np.zeros(len(ctarget))
	for nn in range(len(ctarget)):
		spair = beta_list[csource[nn]]['pair']
		tpair = beta_list[ctarget[nn]]['pair']
		if spair[0] == tpair[1]:
			reciprocal_flag[nn] = 1


	# generate the inputs
	# theoretically, now the values in Mconn can describe a consistent network
	# pick time-courses for latent inputs
	tsize = 40
	nruns_per_person = 5
	tsize_full = tsize*nruns_per_person
	signal = 0.005

	Mintrinsic = np.zeros((Nintrinsic, tsize_full))
	nblocks = [3,7,1,5,3]
	block_length = [5,3,12,4,7]
	magnitude = [1.0, 1.5, 0.5, 0.2, 1.2]

	for nn in range(Nintrinsic):
		p1 = generate_block_paradigm(magnitude[nn]*signal, tsize, nblocks[nn], block_length[nn])
		p1 -= np.mean(p1)
		Mintrinsic[nn, :] = np.tile(p1, nruns_per_person)

	hrf = HRF(TR)  # get ready with the hemodynamic response function

	Mintrinsic_BOLD = np.zeros(np.shape(Mintrinsic))
	for rr in range(Nintrinsic):
		Mintrinsic_BOLD[rr, :] = np.convolve(Mintrinsic[rr, :], hrf, mode='full')[:tsize_full]

	# now change to DCM mode
	A = Mconn[:nregions,:nregions]
	B = np.zeros((nregions,nregions,1))
	C = Mconn[:nregions,-Nintrinsic:]
	D = np.zeros((nregions,nregions,0))

	beta = 0.4*np.random.randn(len(ctarget2))
	A[ctarget2,csource2] = copy.deepcopy(beta)

	ct,cs = np.where(C > 0)
	delta = np.ones(len(ct))
	C[ct,cs] = copy.deepcopy(delta)

	Mconn = np.zeros((nbeta,nbeta))
	Mconn[:nregions,:nregions] = copy.deepcopy(A)
	Mconn[:nregions,-Nintrinsic:] = copy.deepcopy(C)
	for nn in range(Nintrinsic):
		Mconn[-nn-1,-nn-1] = 1

	transit = np.random.randn(nregions,1)
	epsilon = np.array([[0.32237216]])
	decay = np.array([[-0.37869643]])
	Ep = {'A':A, 'B':B, 'C':C, 'D':D, 'transit':transit, 'decay':decay, 'epsilon':epsilon}

	ROInames = [network[xx]['target'] for xx in range(len(network))]   # pick something for setup
	xY = []
	nR = len(ROInames)
	for rr in range(nR):
		# get voxel coordinates for each region - do this later, for now this is just simulation
		coords = [0,0,0]
		xY.append({'coords':coords})

	X0 = 0  # no idea what this is, something about the voxel data

	stim_names = ['paradigm{}'.format(xx) for xx in range(Nintrinsic)]

	delays = np.tile(TR,(nR,1))/2  # equivalent to matlab function
	Q = []
	for nn in range(nR):
		q = np.zeros((nR,nR))
		q[nn,nn] = tsize
		Q += [q]

	# Y = {'dt':TR, 'X0':X0, 'y':Sin_BOLD.T, 'name':ROInames, 'Q':Q}
	Y = {'dt':TR, 'X0':X0, 'name':ROInames, 'Q':Q}
	U = {'dt':TR,'u':Mintrinsic.T, 'name':stim_names, 'idx':[1,0]}   # guessing at idx

	delays = TR*np.ones((nregions,1))
	f = dcm.spm_funcheck('spm_fx_fmri')
	g = dcm.spm_funcheck('spm_gx_fmri')

	# M = {'Nmax':128, 'delays':delays, 'TE':TE, 'IS':'spm_int', 'f':f, 'g':'spm_gx_fmri',  'x':[0],
	# 	 'pE':Ep, 'pC':pC, 'hE':hE, 'hC':hC, 'm':m, 'n':n, 'l':l, 'N':N, 'dt':TR, 'ns':ns,
	# 	 'nograph':1, 'noprint':1, 'P':P}

	x = np.zeros((nregions,5))
	M = {'Nmax':128, 'delays':delays, 'IS':'spm_int', 'f':f, 'g':'spm_gx_fmri', 'x':x, 'm':Nintrinsic, 'l':nregions}

	Sin_BOLD = dcm.spm_int(Ep, M, U).T

	noise = 0.001 * signal * np.random.randn(nregions, tsize * nruns_per_person)  # add some random noise
	Sin_BOLD += noise

	return Sin_BOLD, Mintrinsic, Mintrinsic_BOLD, \
		   network, sem_region_list, beta_list, fintrinsic_count, vintrinsic_count, hrf, \
		   csource, ctarget, dsource, dtarget, nruns_per_person, tsize_full, Mconn, Minput, \
		   latent_flag, reciprocal_flag, fintrinsic_count, vintrinsic_count


def generate_block_paradigm(magnitude, tsize, nblocks, block_length):
	stimvols = nblocks*block_length
	nostimvols = tsize - stimvols
	nostim_length = np.floor(nostimvols/(nblocks+1))

	p = np.zeros(tsize)

	onsets = []
	for nn in range(nblocks):
		oo = (nostim_length + nn*(block_length+nostim_length)).astype(int)
		onsets += [oo]
		p[oo:(oo+block_length)] = 1.0

	p -= np.mean(p)
	p *= magnitude
	return p


#---------HRF----------------------------------------
#-------------------------------------------------------------------
def HRF(TR=2, length=32, peak_delay=6, under_delay=16,
                  peak_disp=1, under_disp=1, p_u_ratio=6,  normalize=True):
    # the hemodynamic response function is the sum of two gamma functions, as defined in SPM

    t = np.linspace(0, length, np.ceil(length/TR).astype('int'))
    if len([v for v in [peak_delay, peak_disp, under_delay, under_disp]
            if v <= 0]):
        raise ValueError("delays and dispersions must be > 0")
    # gamma.pdf only defined for t > 0
    hrf = np.zeros(t.shape, dtype=float)
    pos_t = t[t > 0]
    peak = sps.gamma.pdf(pos_t,
                         peak_delay / peak_disp,
                         loc=0,
                         scale=peak_disp)
    undershoot = sps.gamma.pdf(pos_t,
                               under_delay / under_disp,
                               loc=0,
                               scale=under_disp)
    hrf[t > 0] = peak - undershoot / p_u_ratio
    if not normalize:
        return hrf
    return hrf / np.max(hrf)



def sem_physio_modelV3_copy(Sinput, tsize, Minput, Mintrinsic, Mconn, nruns_per_person, vintrinsic_count, fintrinsic_count, network,
	 					ctarget, csource, dtarget, dsource, timepoint, epoch, latent_flag, reciprocal_flag,
                      	betascale = 0.1, Lweight = 1.0, normalize_var=False, nitermax = 250, verbose = True, initial_nitermax_stage1 = 15,
                      	initial_nsteps_stage1 = 15, beta_initial_vals = []):

	include_stage1b = False

	if fintrinsic_count > 0:
		fintrinsic1 = copy.deepcopy(Mintrinsic[0,:])
	else:
		fintrinsic1 = []

	starttime = time.ctime()
	# initialize gradient-descent parameters--------------------------------------------------------------
	initial_alpha = 1e-3
	initial_Lweight = copy.deepcopy(Lweight)
	initial_dval = 0.05
	alpha_limit = 1.0e-6
	repeat_limit = 2

	# SAPMparams = np.load(SAPMparametersname, allow_pickle=True).flat[0]
	# # load the data values
	# betanamelist = SAPMparams['betanamelist']
	# beta_list = SAPMparams['beta_list']
	# nruns_per_person = SAPMparams['nruns_per_person']
	# nclusterstotal = SAPMparams['nclusterstotal']
	# rnamelist = SAPMparams['rnamelist']
	# nregions = SAPMparams['nregions']
	# cluster_properties = SAPMparams['cluster_properties']
	# cluster_data = SAPMparams['cluster_data']
	# network = SAPMparams['network']
	# fintrinsic_count = SAPMparams['fintrinsic_count']
	# vintrinsic_count = SAPMparams['vintrinsic_count']
	# sem_region_list = SAPMparams['sem_region_list']
	# nclusterlist = SAPMparams['nclusterlist']
	# tsize = SAPMparams['tsize']
	# tplist_full = SAPMparams['tplist_full']
	# tcdata_centered = SAPMparams['tcdata_centered']
	# tcdata_centered_original = SAPMparams['tcdata_centered_original']
	# ctarget = SAPMparams['ctarget']
	# csource = SAPMparams['csource']
	# dtarget = SAPMparams['dtarget']
	# dsource = SAPMparams['dsource']
	# fintrinsic_region = SAPMparams['fintrinsic_region']
	# Mconn = SAPMparams['Mconn']
	# Minput = SAPMparams['Minput']
	# timepoint = SAPMparams['timepoint']
	# epoch = SAPMparams['epoch']
	# latent_flag = SAPMparams['latent_flag']
	# reciprocal_flag = SAPMparams['reciprocal_flag']
	# DBname = SAPMparams['DBname']
	# DBnum = SAPMparams['DBnum']

	regular_flag = 1-latent_flag   # flag where connections are not latent

	# ntime, NP = np.shape(tplist_full)
	Nintrinsics = vintrinsic_count + fintrinsic_count

	ncomponents_to_fit = copy.deepcopy(nregions)
	#---------------------------------------------------------------------------------------------------------
	#---------------------------------------------------------------------------------------------------------
	# repeat the process for each participant-----------------------------------------------------------------
	betalimit = 3.0
	epochnum = 0
	SAPMresults = []
	first_pass_results = []
	second_pass_results = []
	beta_init_record = []

	# one person at a time with this method
	# for nperson in range(NP):
	nruns = nruns_per_person

	# get principal components of Sinput--------------------------
	nr = np.shape(Sinput)[0]
	pca = sklearn.decomposition.PCA()
	pca.fit(Sinput)
	components = pca.components_
	loadings = pca.transform(Sinput)
	mu2 = np.mean(Sinput, axis=0)
	loadings = np.concatenate((np.ones((nr, 1)), loadings), axis=1)
	components = np.concatenate((mu2[np.newaxis, :], components), axis=0)

	beta_int1 = 0.1
	lastgood_beta_int1 = copy.deepcopy(beta_int1)

	# initialize beta values-----------------------------------
	nbeta = len(csource)
	if isinstance(betascale,str):
		if betascale == 'shotgun':
			beta_initial = betaval_init_shotgun(initial_Lweight, csource, ctarget, Sinput, Minput, Mconn, components,
								loadings, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
												ncomponents_to_fit, nreps=10000)

			beta_initial = beta_initial[np.newaxis,:]
			nitermax_stage1 = 0
		else:
			# read saved beta_initial values
			b = np.load(betascale,allow_pickle=True).flat[0]
			beta_initial = b['beta_initial']
			beta_initial = beta_initial[np.newaxis,:]
			nitermax_stage1 = 0
		nsteps_stage1 = 1
	else:
		nsteps_stage1 = copy.deepcopy(initial_nsteps_stage1)
		beta_initial = betascale*np.random.randn(nsteps_stage1,nbeta)
		nregion,ntotal = np.shape(Minput)
		cl = np.where(latent_flag > 0)[0]
		beta_initial[:,cl] = 1.0

		if len(beta_initial_vals) == nbeta:
			beta_initial[0,:] = copy.deepcopy(beta_initial_vals)

		nitermax_stage1 = copy.deepcopy(initial_nitermax_stage1)

	# initialize deltavals
	delta_initial = np.ones(len(dtarget))
	deltascale = np.std(Sinput,axis=1)
	meanscale = np.mean(deltascale)

	# initialize
	results_record = []
	ssqd_record = []

	# stage 1 - test the initial betaval settings
	if include_stage1b:
		stage1_ssqd = np.zeros(2*nsteps_stage1)
	else:
		stage1_ssqd = np.zeros(nsteps_stage1)
	stage1_results = []
	for ns in range(nsteps_stage1):
		ssqd_record_stage1 = []
		beta_init_record.append({'beta_initial':beta_initial[ns,:]})

		# initalize Sconn
		betavals = copy.deepcopy(beta_initial[ns,:]) # initialize beta values at zero
		lastgood_betavals = copy.deepcopy(betavals)
		deltavals = copy.deepcopy(delta_initial)
		lastgood_deltavals = copy.deepcopy(deltavals)

		alphalist = initial_alpha*np.ones(nbeta)
		alphabint = copy.deepcopy(initial_alpha)
		alpha = copy.deepcopy(initial_alpha)
		Lweight = copy.deepcopy(initial_Lweight)
		dval = copy.deepcopy(initial_dval)

		# # starting point for optimizing intrinsics with given betavals----------------------------------------------------
		Mconn[ctarget,csource] = copy.deepcopy(betavals)
		Minput[dtarget, dsource] = copy.deepcopy(deltavals)
		fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput, Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1, ncomponents_to_fit)
		# Soutput = Meigv @ Mintrinsic  # signalling over each connection
		ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv

		ssqd_starting = copy.deepcopy(ssqd)
		ssqd_old = copy.deepcopy(ssqd)
		ssqd_record += [ssqd]

		iter = 0
		converging = True
		dssq_record = np.ones(3)
		dssq_count = 0
		sequence_count = 0

		while alpha > alpha_limit and iter < nitermax_stage1 and converging:
			iter += 1

			betavals, deltavals, beta_int1, fit, dssq_db, dssq_dd, dssq_dbeta1, ssqd_original, ssqd, alpha, alphabint = \
												pysapm.update_betavals_V3(Sinput, components, loadings, Minput, Mconn, betavals,
												deltavals, betalimit, ctarget, csource, dtarget, dsource,
												dval,fintrinsic_count,
												vintrinsic_count, beta_int1,fintrinsic1, Lweight, regular_flag, alpha,alphabint,
												ncomponents_to_fit, latent_flag=latent_flag)  # kappavals, ktarget, ksource,
			betavals[latent_flag > 0] = 1.0

			ssqd_record_stage1 += [ssqd]

			if ssqd > ssqd_original:
				alpha *= 0.5
				alphabint *= 0.5
				betavals = copy.deepcopy(lastgood_betavals)  # no improvement, so don't update
				deltavals = copy.deepcopy(lastgood_deltavals)
				beta_int1 = copy.deepcopy(lastgood_beta_int1)
			else:
				lastgood_betavals = copy.deepcopy(betavals)
				lastgood_deltavals = copy.deepcopy(deltavals)
				lastgood_beta_int1 = copy.deepcopy(beta_int1)

			Mconn[ctarget, csource] = copy.deepcopy(betavals)
			Minput[dtarget, dsource] = copy.deepcopy(deltavals)

			fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput,
												Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
												ncomponents_to_fit)
			# Soutput = Meigv @ Mintrinsic  # signalling over each connection
			ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv

			err_total = Sinput - fit
			Smean = np.mean(Sinput)
			errmean = np.mean(err_total)

			R2list = 1.0 - np.sum((Sinput - fit) ** 2, axis=1) / np.sum(Sinput ** 2, axis=1)
			R2avg = np.mean(R2list)
			R2total = 1.0 - np.sum((Sinput - fit) ** 2) / np.sum(Sinput ** 2)

			results_record.append({'Sinput': Sinput, 'fit': fit, 'Mintrinsic': Mintrinsic, 'Meigv': Meigv})

			ssqchange = ssqd - ssqd_original

			if verbose:
				nperson = 0
				print('SAPM  {} stage1 pass {} iter {} alpha {:.3e}  ssqd {:.2f} error {:.2f} error2 {:.2f} change {:.3f}  percent {:.1f}  R2 avg {:.3f}  R2 total {:.3f}'.format(nperson,
								ns, iter, alpha, ssqd, error, error2, ssqchange, 100.*ssqd/ssqd_starting, R2avg, R2total))
			ssqd_old = copy.deepcopy(ssqd)
			# now repeat it ...
		stage1_ssqd[ns] = ssqd
		stage1_results.append({'betavals':betavals, 'deltavals':deltavals})


	if include_stage1b:
		# # stage 1 part b
		for ns in range(nsteps_stage1):
			ssqd_record_stage1 = []
			bi = beta_initial[ns,:]
			bi[reciprocal_flag > 0] *= -1.0   # invert all the betavals for reciprocal connections and test them
			beta_init_record.append({'beta_initial':bi})

			# initalize Sconn
			betavals = copy.deepcopy(bi) # initialize beta values at zero
			lastgood_betavals = copy.deepcopy(betavals)
			deltavals = copy.deepcopy(delta_initial)
			lastgood_deltavals = copy.deepcopy(deltavals)

			alphalist = initial_alpha*np.ones(nbeta)
			alphabint = copy.deepcopy(initial_alpha)
			alpha = copy.deepcopy(initial_alpha)
			Lweight = copy.deepcopy(initial_Lweight)
			dval = copy.deepcopy(initial_dval)

			# # starting point for optimizing intrinsics with given betavals----------------------------------------------------
			Mconn[ctarget,csource] = copy.deepcopy(betavals)
			Minput[dtarget, dsource] = copy.deepcopy(deltavals)
			fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput, Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1, ncomponents_to_fit)
			# Soutput = Meigv @ Mintrinsic  # signalling over each connection
			ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv

			ssqd_starting = copy.deepcopy(ssqd)
			ssqd_old = copy.deepcopy(ssqd)
			ssqd_record += [ssqd]

			iter = 0
			converging = True
			dssq_record = np.ones(3)
			dssq_count = 0
			sequence_count = 0

			while alpha > alpha_limit and iter < nitermax_stage1 and converging:
				iter += 1

				betavals, deltavals, beta_int1, fit, dssq_db, dssq_dd, dssq_dbeta1, ssqd_original, ssqd, alpha, alphabint = \
													pysapm.update_betavals_V3(Sinput, components, loadings, Minput, Mconn, betavals,
													deltavals, betalimit, ctarget, csource, dtarget, dsource,
													dval,fintrinsic_count,
													vintrinsic_count, beta_int1,fintrinsic1, Lweight, regular_flag, alpha,alphabint,
													ncomponents_to_fit, latent_flag=latent_flag)  # kappavals, ktarget, ksource,

				ssqd_record_stage1 += [ssqd]

				if ssqd > ssqd_original:
					alpha *= 0.5
					alphabint *= 0.5
					betavals = copy.deepcopy(lastgood_betavals)  # no improvement, so don't update
					deltavals = copy.deepcopy(lastgood_deltavals)
					beta_int1 = copy.deepcopy(lastgood_beta_int1)
				else:
					lastgood_betavals = copy.deepcopy(betavals)
					lastgood_deltavals = copy.deepcopy(deltavals)
					lastgood_beta_int1 = copy.deepcopy(beta_int1)

				Mconn[ctarget, csource] = copy.deepcopy(betavals)
				Minput[dtarget, dsource] = copy.deepcopy(deltavals)

				fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput,
													Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
													ncomponents_to_fit)
				# Soutput = Meigv @ Mintrinsic  # signalling over each connection
				ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv

				err_total = Sinput - fit
				Smean = np.mean(Sinput)
				errmean = np.mean(err_total)

				R2list = 1.0 - np.sum((Sinput - fit) ** 2, axis=1) / np.sum(Sinput ** 2, axis=1)
				R2avg = np.mean(R2list)
				R2total = 1.0 - np.sum((Sinput - fit) ** 2) / np.sum(Sinput ** 2)

				results_record.append({'Sinput': Sinput, 'fit': fit, 'Mintrinsic': Mintrinsic, 'Meigv': Meigv})

				ssqchange = ssqd - ssqd_original

				if verbose:
					nperson = 0
					print('SAPM  {} stage1b pass {} iter {} alpha {:.3e}  ssqd {:.2f} error {:.2f} error2 {:.2f} change {:.3f}  percent {:.1f}  R2 avg {:.3f}  R2 total {:.3f}'.format(nperson,
									ns, iter, alpha, ssqd, error, error2, ssqchange, 100.*ssqd/ssqd_starting, R2avg, R2total))
				ssqd_old = copy.deepcopy(ssqd)
				# now repeat it ...
			stage1_ssqd[ns+nsteps_stage1] = ssqd
			stage1_results.append({'betavals':betavals, 'deltavals':deltavals})


	# get the best betavals from stage1 so far ...
	x = np.argmin(stage1_ssqd)
	betavals = stage1_results[x]['betavals']
	deltavals = stage1_results[x]['deltavals']

	# stage 2
	# # starting point for optimizing intrinsics with given betavals----------------------------------------------------
	if verbose: print('starting stage 2 ....')
	lastgood_betavals = copy.deepcopy(betavals)
	alpha = copy.deepcopy(initial_alpha)
	alphabint = copy.deepcopy(initial_alpha)
	Lweight = copy.deepcopy(initial_Lweight)
	dval = copy.deepcopy(initial_dval)

	# # starting point for optimizing intrinsics with given betavals----------------------------------------------------
	Mconn[ctarget, csource] = copy.deepcopy(betavals)
	Minput[dtarget, dsource] = copy.deepcopy(deltavals)

	fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput,
											Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
											ncomponents_to_fit)
	# Soutput = Meigv @ Mintrinsic  # signalling over each connection
	ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv


	ssqd_starting = copy.deepcopy(ssqd)
	ssqd_old = copy.deepcopy(ssqd)
	ssqd_record += [ssqd]

	iter = 0
	converging = True
	dssq_record = np.ones(3)
	dssq_count = 0
	sequence_count = 0

	while alpha > alpha_limit and iter < nitermax and converging:
		iter += 1
		betavals, deltavals, beta_int1, fit, dssq_db, dssq_dd, dssq_dbeta1, ssqd_original, ssqd, alpha, alphabint = \
			pysapm.update_betavals_V3(Sinput, components, loadings, Minput, Mconn, betavals, deltavals, betalimit,
							   ctarget, csource, dtarget, dsource, dval, fintrinsic_count,
							   vintrinsic_count, beta_int1, fintrinsic1, Lweight, regular_flag, alpha, alphabint,
							   ncomponents_to_fit, latent_flag=latent_flag)   #, kappavals, ktarget, ksource

		ssqd_record_stage1 += [ssqd]

		if ssqd > ssqd_original:
			alpha *= 0.5
			alphabint *= 0.5
			betavals = copy.deepcopy(lastgood_betavals)  # no improvement, so don't update
			deltavals = copy.deepcopy(lastgood_deltavals)
			beta_int1 = copy.deepcopy(lastgood_beta_int1)
			sequence_count = 0
		else:
			lastgood_betavals = copy.deepcopy(betavals)
			lastgood_deltavals = copy.deepcopy(deltavals)
			lastgood_beta_int1 = copy.deepcopy(beta_int1)
			sequence_count += 1
			if sequence_count > 3:
				alpha *= 1.3
				alphabint *= 1.3
				sequence_count = 0

		Mconn[ctarget, csource] = copy.deepcopy(betavals)
		Minput[dtarget, dsource] = copy.deepcopy(deltavals)

		fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput,
											Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
											ncomponents_to_fit)
		# Soutput = Meigv @ Mintrinsic  # signalling over each connection
		ssqd, error, error2, costfactor = pysapm.sapm_error_function_V3(Sinput, Mconn, fit, loadings, loadings_fit, Lweight, betavals, deltavals, regular_flag)  # , deltavals, beta_int1, Minput, Mintrinsic, Meigv

		err_total = Sinput - fit
		Smean = np.mean(Sinput)
		errmean = np.mean(err_total)

		R2list = 1.0 - np.sum((Sinput - fit) ** 2, axis=1) / np.sum(Sinput ** 2, axis=1)
		R2avg = np.mean(R2list)
		R2total = 1.0 - np.sum((Sinput - fit) ** 2) / np.sum(Sinput ** 2)

		results_record.append({'Sinput': Sinput, 'fit': fit, 'Mintrinsic': Mintrinsic, 'Meigv': Meigv})

		ssqchange = ssqd - ssqd_original

		if verbose:
			nperson = 0
			print('SAPM  {} beta vals:  iter {} alpha {:.3e}  ssqd {:.2f} error {:.2f} error2 {:.2f} change {:.3f}  percent {:.1f}  R2 avg {:.3f}  R2 total {:.3f}'.format(
					nperson,iter, alpha, ssqd, error, error2, ssqchange, 100. * ssqd / ssqd_starting, R2avg, R2total))
		ssqd_old = copy.deepcopy(ssqd)
		# now repeat it ...

	# fit the results now to determine output signaling from each region
	Mconn[ctarget, csource] = copy.deepcopy(betavals)
	Minput[dtarget, dsource] = copy.deepcopy(deltavals)
	fit, loadings_fit, W, Mintrinsic, Meigv, err = pysapm.network_eigenvector_method_V3(Sinput, components, loadings, Minput,
										Mconn, fintrinsic_count, vintrinsic_count, beta_int1, fintrinsic1,
										ncomponents_to_fit)

	Sconn = Meigv @ Mintrinsic    # signalling over each connection

	entry = {'Sinput':Sinput, 'Sconn':Sconn, 'beta_int1':beta_int1, 'Mconn':Mconn, 'Minput':Minput,
			 'fit':fit, 'loadings_fit':loadings_fit, 'W':W, 'loadings':loadings, 'components':components,
			 'R2total':R2total, 'R2avg':R2avg, 'Mintrinsic':Mintrinsic, 'fintrinsic_count':fintrinsic_count, 'vintrinsic_count':vintrinsic_count,
			 'Meigv':Meigv, 'betavals':betavals, 'deltavals':deltavals, 'fintrinsic1':fintrinsic1}

	SAPMresults = copy.deepcopy(entry)

	return SAPMresults


def test_SAPM_performance(network, tsize, nruns_per_person, Mconn, Minput,
                          Sinput, Soutput, beta_list, ctarget, csource, dtarget, dsource,
                          vintrinsic_count, fintrinsic_count, Mintrinsic, latent_flag, reciprocal_flag):
    # tsize = 40
    # # networkmodel_name = generate_random_networkfile(networkmodel_name, nregions, fintrinsic_count, vintrinsic_count, tsize, nclusters)
    # networkmodel_name = r'E:\SAPMresults_Dec2022\network_model_June2023_SAPM_test2.xlsx'
    #
    # tsize = 200
    # nruns_per_person = 5
    # dataset = generate_simulated_data_and_results(networkmodel_name, tsize, nruns_per_person, TR)

    # test the same data repeatedly
    # if nn == 0:
    # 	saved_dataset = copy.deepcopy(dataset)
    # else:
    # 	dataset = copy.deepcopy(saved_dataset)

    # Mconn = copy.deepcopy(dataset['Mconn'])
    # ctarget = copy.deepcopy(dataset['ctarget'])
    # csource = copy.deepcopy(dataset['csource'])

    # run SAPM with the simulated network and data ...
    # create a network model
    timepoint = 'all'
    epoch = 'all'
    betascale = 1e-2

    # SAPMresultsname = os.path.join(resultsdir, 'sim_results_sim{}.npy'.format(nn))
    # SAPMparametersname = os.path.join(resultsdir, 'sim_params_sim{}.npy'.format(nn))

    # Sinput = copy.deepcopy(dataset['Sinput'])
    # Minput = copy.deepcopy(dataset['Minput'])
    # Sconn = copy.deepcopy(dataset['Sconn'])
    # Mconn = copy.deepcopy(dataset['Mconn'])
    # Mintrinsic = copy.deepcopy(dataset['Mintrinsic'])
    # ctarget = copy.deepcopy(dataset['ctarget'])
    # csource = copy.deepcopy(dataset['csource'])
    # dtarget = copy.deepcopy(dataset['dtarget'])
    # dsource = copy.deepcopy(dataset['dsource'])
    # beta_list = copy.deepcopy(dataset['beta_list'])
    # latent_flag = copy.deepcopy(dataset['latent_flag'])
    # reciprocal_flag = copy.deepcopy(dataset['reciprocal_flag'])

    # Sinput_base = copy.deepcopy(dataset['Sinput_base'])
    # Sconn_base = copy.deepcopy(dataset['Sconn_base'])
    # Mintrinsic_base = copy.deepcopy(dataset['Mintrinsic_base'])

    nregions, tsize_full = np.shape(Sinput)
    # noise_level = 0.05
    # SinputN = Sinput + noise_level * np.random.randn(nregions, tsize_full)
    # Sinput_baseN = Sinput_base + noise_level * np.random.randn(nregions, tsize_full)

    Mconn_original = copy.deepcopy(Mconn)
    Minput_input = copy.deepcopy(Minput)
    Mconn_input = copy.deepcopy(Mconn)
    Soutput_original = copy.deepcopy(Soutput)
    Mconn_input[ctarget, csource] = 0.0
    # print('\n\nsim {} of {}'.format(nn, nsamples))
    SAPMresults = sem_physio_modelV3_copy(Sinput, tsize, Minput, Mintrinsic, Mconn, nruns_per_person, vintrinsic_count,
                                          fintrinsic_count, network, ctarget, csource, dtarget, dsource,
                                          timepoint, epoch, latent_flag, reciprocal_flag,
                                          betascale=1e-2, Lweight=0.01, normalize_var=False, nitermax=400, verbose=True,
                                          initial_nitermax_stage1=200,
                                          initial_nsteps_stage1=25, beta_initial_vals=[])


    SAPMresults['Mconn_model'] = copy.deepcopy(Mconn_original)
    SAPMresults['Minput_model'] = copy.deepcopy(Minput_input)
    SAPMresults['Mintrinsic_model'] = copy.deepcopy(Mintrinsic)
    SAPMresults['Sconn_model'] = copy.deepcopy(Soutput_original)
    SAPMresults['ctarget'] = copy.deepcopy(ctarget)
    SAPMresults['csource'] = copy.deepcopy(csource)
    SAPMresults['dtarget'] = copy.deepcopy(dtarget)
    SAPMresults['dsource'] = copy.deepcopy(dsource)
    SAPMresults['Sinput_model'] = copy.deepcopy(Sinput)

    return SAPMresults




def test_DCM_performance(network, tsize, nruns_per_person,
						Mconn, Minput, Sin, beta_list, ctarget, csource,
						dtarget, dsource, Mintrinsic, latent_flag,
						reciprocal_flag):

	TR = 4.0
	ROInames = [network[xx]['target'] for xx in range(len(network))]   # pick something for setup
	xY = []
	nR = len(ROInames)
	for rr in range(nR):
		# get voxel coordinates for each region - do this later, for now this is just simulation
		coords = [0,0,0]
		xY.append({'coords':coords})

	X0 = 0  # no idea what this is, something about the voxel data

	tsize_full_check, Nintrinsic = np.shape(Mintrinsic)
	stim_names = ['paradigm{}'.format(xx) for xx in range(Nintrinsic)]

	delays = np.tile(TR,(nR,1))/2  # equivalent to matlab function
	Q = []
	for nn in range(nR):
		q = np.zeros((nR,nR))
		q[nn,nn] = tsize
		Q += [q]

	# Q = np.ones((nR,1))*tsize

	Y = {'dt':TR, 'X0':X0, 'y':Sin, 'name':ROInames, 'Q':Q}
	U = {'dt':TR,'u':Mintrinsic, 'name':stim_names, 'idx':[1,0]}   # guessing at idx

	TE = 0.030

	options = {'nonlinear':0, 'two_state':0, 'stochastic':0, 'centre':0, 'noprint':0, 'nograph':1, 'maxnodes':10, 'maxit':128, 'hidden':[], 'induced':0}

	# connectivity matrices  ???
	# a = np.array([[1,1,1,0],[1,1,0,1],[1,0,1,1],[0,1,1,1]])
	# b0 = np.zeros((nR,nR))   # this example has two experimental factors that could modulate connectivity
	# b1 = np.array([[1,0,1,0], [0,1,0,1],[1,0,1,0],[0,1,0,1]])
	# b = np.concatenate((b0[:,:,np.newaxis],b1[:,:,np.newaxis]),axis=2)
	# c = np.array([[1,0],[1,0],[1,0],[1,0]])
	d = np.zeros((nR,nR,0))

	A = copy.deepcopy(Mconn)
	A[ctarget,csource] = 1
	a = A[:nR, :nR]
	b = np.zeros(np.shape(a))
	c = A[:nR, -Nintrinsic:]

	P = {'xY':xY, 'n':nR, 'v':tsize, 'U':U, 'Y':Y, 'delays':delays, 'TE':TE, 'options':options,'a':a,'b':b,'c':c,'d':d}
	DCM = dcm.spm_dcm_estimate(P)

	# results = {'DCM':DCM}
	# np.save(resultsname, results)

	return DCM


def generate_sim_data_V2():
    # try to figure out how to generate "biogically realistic" data sets
    #  1) pick a network model (based on SEM? SAPM? DCM?)
    #  2) pick connectivity values
    #  3) pick latent inputs
    #  4) generate BOLD responses that fit this model realistically
    #  instead ==> do NOT base the model on one of the connectivity analysis methods
    #               base it on physiology

	save_data_file = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\network_test_data10.npy'
	save_mat_data_file = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\network_test_data10.mat'
	fmri_model_data_file = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\fmri_model_data10.npy'

	# networkfile = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\network_model_SAPM.xlsx'
	networkfile = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\network_model_10.xlsx'
	test_data_filename = r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_results10.npy'

	simulate_new_network = True
	use_saved_fMRI_data = False

	ntrials = 1
	repeated_run_data = []

	for nt in range(ntrials):
		print('\n--------------------------------------------')
		print('trial number {}'.format(nt))
		print('--------------------------------------------\n')

		try:

			if simulate_new_network:
				network, nclusterlist, sem_region_list, fintrinsic_count, vintrinsic_count, fintrinsic_base = pysapm.load_network_model_w_intrinsics(networkfile)
				Nintrinsic = fintrinsic_count + vintrinsic_count
				nbeta = len(nclusterlist)
				nregions = nbeta - Nintrinsic
				networktargetlist = [network[x]['target'] for x in range(len(network))]

				use_sapm_sim_method = True

				if use_sapm_sim_method:
					Sin_BOLD, Sin_neural, Sout_BOLD, Sout_neural, Mintrinsic, Mintrinsic_BOLD, \
					network, sem_region_list, beta_list, fintrinsic_count, vintrinsic_count, hrf, \
					csource, ctarget, dsource, dtarget, nruns_per_person, tsize_full, Mconn, Minput, \
					latent_flag, reciprocal_flag, fintrinsic_count, vintrinsic_count = define_network_data_based_on_SAPM_method(networkfile)
				else:
					Sin_BOLD, Mintrinsic, Mintrinsic_BOLD, \
					network, sem_region_list, beta_list, fintrinsic_count, vintrinsic_count, hrf, \
					csource, ctarget, dsource, dtarget, nruns_per_person, tsize_full, Mconn, Minput, \
					latent_flag, reciprocal_flag, fintrinsic_count, vintrinsic_count = define_network_data_based_on_DCM_method(networkfile)
					Sout_BOLD = np.zeros(np.shape(Sin_BOLD))
					Sin_neural = np.zeros(np.shape(Sin_BOLD))
					Sout_neural = np.zeros(np.shape(Sin_BOLD))

					Mconn_model_DCM = copy.deepcopy(Mconn)

				test_data = {'network':network, 'tsize':tsize_full, 'nruns_per_person':nruns_per_person,
							 'Mconn':Mconn, 'Minput':Minput, 'Sin_BOLD':Sin_BOLD, 'Sout_BOLD':Sout_BOLD,
							 'Sin_neural': Sin_neural, 'Sout_neural': Sout_neural,
							 'beta_list':beta_list, 'ctarget':ctarget, 'csource':csource,
							'dtarget':dtarget, 'dsource':dsource, 'Mintrinsic_BOLD':Mintrinsic_BOLD, 'Mintrinsic':Mintrinsic,
							 'latent_flag':latent_flag, 'reciprocal_flag':reciprocal_flag,
							 'fintrinsic_count':fintrinsic_count, 'vintrinsic_count':vintrinsic_count}

				np.save(save_data_file, test_data)

				savemat(save_mat_data_file, test_data)

			else:
				test_data = np.load(save_data_file, allow_pickle=True).flat[0]
				network = test_data['network']
				tsize_full = test_data['tsize']
				nruns_per_person = test_data['nruns_per_person']
				Mconn = test_data['Mconn']
				Minput = test_data['Minput']
				Sin_BOLD = test_data['Sin_BOLD']
				Sout_BOLD = test_data['Sout_BOLD']
				Sin_neural = test_data['Sin_neural']
				Sout_neural = test_data['Sout_neural']
				beta_list = test_data['beta_list']
				ctarget = test_data['ctarget']
				csource = test_data['csource']
				dtarget = test_data['dtarget']
				dsource = test_data['dsource']
				fintrinsic_count = test_data['fintrinsic_count']
				vintrinsic_count = test_data['vintrinsic_count']
				Mintrinsic = test_data['Mintrinsic']
				Mintrinsic_BOLD = test_data['Mintrinsic_BOLD']
				latent_flag = test_data['latent_flag']
				reciprocal_flag = test_data['reciprocal_flag']

			if use_saved_fMRI_data:
				fmri_model_data = np.load(fmri_model_data_file, allow_pickle=True).flat[0]

				tsize_full = fmri_model_data['tsize']
				nruns_per_person = fmri_model_data['nruns_per_person']
				Sin_BOLD = fmri_model_data['Sin_BOLD']
				Sout_BOLD = fmri_model_data['Sout_BOLD']
				Sin_neural = fmri_model_data['Sin_neural']
				Sout_neural = fmri_model_data['Sout_neural']
				Mintrinsic_BOLD = fmri_model_data['Mintrinsic_BOLD']
				Mintrinsic = fmri_model_data['Mintrinsic']

			else:
				fmri_model_data = {'tsize': tsize_full, 'nruns_per_person': nruns_per_person,
								   'Sin_BOLD': Sin_BOLD, 'Sout_BOLD': Sout_BOLD, 'Sin_neural': Sin_neural, 'Sout_neural': Sout_neural,
								   'Mintrinsic_BOLD':Mintrinsic_BOLD, 'Mintrinsic':Mintrinsic}
				np.save(fmri_model_data_file, fmri_model_data)


			SAPMresults = test_SAPM_performance(network, tsize_full, nruns_per_person,
												Mconn, Minput, Sin_BOLD, Sout_BOLD, beta_list, ctarget, csource,
												dtarget, dsource, vintrinsic_count, fintrinsic_count,
												Mintrinsic_BOLD, latent_flag, reciprocal_flag)

			# SAPMresultsname = os.path.join(resultsdir, 'sim_results_sim{}.npy'.format(nn))
			# SAPMparametersname = os.path.join(resultsdir, 'sim_params_sim{}.npy'.format(nn))

			DCMresults = test_DCM_performance(network, tsize_full, nruns_per_person,
												Mconn, Minput, Sin_BOLD.T, beta_list, ctarget, csource,
												dtarget, dsource, Mintrinsic.T, latent_flag,
												reciprocal_flag)

			results = {'SAPMresults':SAPMresults, 'DCMresults':DCMresults}
			resultsfilename = r'C:\stroman\papers_in_progress\methods_comparison\method_comparison_results10.npy'
			np.save(resultsfilename, results)


			# modeled input signaling in DCMresults is DCMresults['y']
			# modeled network parameters in DCMresults are in DCMresults['Ep']

			Sin_dcm = copy.deepcopy(DCMresults['y'])
			Sin_sapm = copy.deepcopy(SAPMresults['Sinput'])
			Sout_sapm = copy.deepcopy(SAPMresults['Sconn'])
			Sin_model = copy.deepcopy(SAPMresults['Sinput_model'])

			A = copy.deepcopy(DCMresults['Ep']['A'])
			C = copy.deepcopy(DCMresults['Ep']['C'])
			AC = np.concatenate((A,C),axis=1)

			Mconn = copy.deepcopy(SAPMresults['Mconn'])
			Mconn_model = copy.deepcopy(SAPMresults['Mconn_model'])

			if use_sapm_sim_method:
				Mconn_model = copy.deepcopy(SAPMresults['Mconn_model'])
			else:
				Mconn_model = copy.deepcopy(Mconn_model_DCM)

			ctarget = copy.deepcopy(SAPMresults['ctarget'])
			csource = copy.deepcopy(SAPMresults['csource'])

			ctarget2 = []
			csource2 = []
			for nn in range(len(ctarget)):
				if csource[nn] < nregions:
					ctarget2 += [ctarget[nn]]
					csource2 += [csource[nn]]

			# try generating data based on DCM method
			Ep = copy.deepcopy(DCMresults['Ep'])
			M = copy.deepcopy(DCMresults['M'])
			U = copy.deepcopy(DCMresults['U'])
			ytest = dcm.spm_int(Ep, M, U)

			entry = {'Sin_dcm':Sin_dcm, 'Sin_sapm':Sin_sapm, 'Sout_sapm':Sout_sapm,
					 'Sin_model':Sin_model, 'A':A, 'C':C, 'AC':AC, 'Mconn':Mconn,
					 'Mconn_model':Mconn_model, 'ctarget':ctarget, 'csource':csource,
					 'csource2':csource2, 'ctarget2':ctarget2}
			repeated_run_data.append(entry)

			np.save(test_data_filename, repeated_run_data)

		except:
			print('\n--------------------------------------------')
			print('trial {} did not work out'.format(nt))
			print('--------------------------------------------\n')

	# for loading data and showing it
	repeated_run_data = np.load(test_data_filename, allow_pickle=True)
	ntrials = len(repeated_run_data)
	ctarget = repeated_run_data[0]['ctarget']
	csource = repeated_run_data[0]['csource']
	ctarget2 = repeated_run_data[0]['ctarget2']
	csource2 = repeated_run_data[0]['csource2']

	for nt in range(ntrials):
		if nt == 0:
			A_total = copy.deepcopy(repeated_run_data[nt]['A'][ctarget2,csource2])
			Mconn_total = copy.deepcopy(repeated_run_data[nt]['Mconn'][ctarget2,csource2])
			Mconnmodel_total = copy.deepcopy(repeated_run_data[nt]['Mconn_model'][ctarget2,csource2])
		else:
			A_total = np.concatenate((A_total, repeated_run_data[nt]['A'][ctarget2,csource2]))
			Mconn_total = np.concatenate((Mconn_total, repeated_run_data[nt]['Mconn'][ctarget2,csource2]))
			Mconnmodel_total = np.concatenate((Mconnmodel_total, repeated_run_data[nt]['Mconn_model'][ctarget2,csource2]))

	print('finished running repeated trials.')



def display_results(Sin_model, Sin_sapm, Sin_dcm, Mconn_model, Mconn, AC):
	windownum = 21

	plt.close(windownum)
	fig = plt.figure(windownum)
	plt.plot(range(tsize_full), Sin_model[0,:], '-xg')
	plt.plot(range(tsize_full), Sin_sapm[0,:], '-ob')
	plt.plot(range(tsize_full), Sin_dcm[:,0], '-r')

	plt.close(windownum+1)
	fig = plt.figure(windownum+1)
	plt.plot(Mconn_model[:nregions,:nregions].flatten(), Mconn[:nregions,:nregions].flatten(), 'xr')
	plt.plot([-0.7,0.7],[-0.7,0.7],'-k')
	plt.plot(Mconn_model[:nregions,:nregions].flatten(), AC[:nregions,:nregions].flatten(), 'ob')


def display_results2(Mconnmodel_total, Mconn_total, A_total):
	windownum = 31

	plt.close(windownum)
	fig = plt.figure(windownum)
	plt.plot(Mconnmodel_total, Mconn_total, 'og')
	plt.plot([-0.7,0.7],[-0.7,0.7],'-k')

	plt.close(windownum+1)
	fig = plt.figure(windownum+1)
	plt.plot(Mconnmodel_total, A_total, 'xr')
	plt.plot([-0.7,0.7],[-0.7,0.7],'-k')


def compare_stats():
	test_data_filename_list = [r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_results2.npy',
						  r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_results.npy',
						  r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_results3.npy',
						  r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_resultsD2.npy',
						  r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_resultsD.npy',
						  r'C:\Users\16135\PycharmProjects\pantheon\venv\test_functions\repeated_runs_resultsD3.npy']

	simtype = ['sapm', 'sapm', 'sapm', 'dcm', 'dcm', 'dcm']
	nregions = [3, 4, 5, 3, 4, 5]

	for nf,fname in enumerate(test_data_filename_list):
		repeated_run_data = np.load(fname, allow_pickle=True)
		ntrials = len(repeated_run_data)
		ctarget = repeated_run_data[0]['ctarget']
		csource = repeated_run_data[0]['csource']
		ctarget2 = repeated_run_data[0]['ctarget2']
		csource2 = repeated_run_data[0]['csource2']

		for nt in range(ntrials):
			if nt == 0:
				A_total = copy.deepcopy(repeated_run_data[nt]['A'][ctarget2,csource2])
				Mconn_total = copy.deepcopy(repeated_run_data[nt]['Mconn'][ctarget2,csource2])
				Mconnmodel_total = copy.deepcopy(repeated_run_data[nt]['Mconn_model'][ctarget2,csource2])
			else:
				A_total = np.concatenate((A_total, repeated_run_data[nt]['A'][ctarget2,csource2]))
				Mconn_total = np.concatenate((Mconn_total, repeated_run_data[nt]['Mconn'][ctarget2,csource2]))
				Mconnmodel_total = np.concatenate((Mconnmodel_total, repeated_run_data[nt]['Mconn_model'][ctarget2,csource2]))

		Scomp = np.corrcoef(Mconnmodel_total, Mconn_total)
		Dcomp = np.corrcoef(Mconnmodel_total, A_total)

		print('\n\nnf = {} data:  {}'.format(nf, fname))
		print('{} model simulation,  {} regions'.format(simtype[nf],nregions[nf]))
		print('A vs model   R = {:.4f}'.format(Dcomp[0,1]))
		print('M vs model   R = {:.4f}'.format(Scomp[0,1]))


def DCM_on_dataset():

	fname = r'E:\SAPM_DCM_comparison\AllPain_1202023213_params.npy'
	# network model can be slightly different than the one used for SAPM,
	# particularly how latent inputs are defined
	neworkmodel_DCM = r'E:\SAPM_DCM_comparison\network_model_DCM.xlsx'
	cnums = [1, 2, 0, 2, 0, 2, 3, 2, 1, 3]


	params = np.load(fname, allow_pickle=True).flat[0]
	tcdata_centered = copy.deepcopy(params['tcdata_centered'])
	cluster_properties = copy.deepcopy(params['cluster_properties'])
	cluster_data = copy.deepcopy(params['cluster_data'])
	csource = copy.deepcopy(params['csource'])
	ctarget = copy.deepcopy(params['ctarget'])
	dsource = copy.deepcopy(params['dsource'])
	dtarget = copy.deepcopy(params['dtarget'])
	epoch = copy.deepcopy(params['epoch'])
	timepoint = copy.deepcopy(params['timepoint'])
	betanamelist = copy.deepcopy(params['betanamelist'])
	beta_list = copy.deepcopy(params['beta_list'])
	nruns_per_person = copy.deepcopy(params['nruns_per_person'])
	nclusterstotal = copy.deepcopy(params['nclusterstotal'])
	rnamelist = copy.deepcopy(params['rnamelist'])
	nregions = copy.deepcopy(params['nregions'])
	network = copy.deepcopy(params['network'])
	fintrinsic_count = copy.deepcopy(params['fintrinsic_count'])
	vintrinsic_count = copy.deepcopy(params['vintrinsic_count'])
	fintrinsic_region = copy.deepcopy(params['fintrinsic_region'])
	sem_region_list = copy.deepcopy(params['sem_region_list'])
	nclusterlist = copy.deepcopy(params['nclusterlist'])
	tsize = copy.deepcopy(params['tsize'])
	tplist_full = copy.deepcopy(params['tplist_full'])
	Mconn = copy.deepcopy(params['Mconn'])
	Minput = copy.deepcopy(params['Minput'])
	latent_flag = copy.deepcopy(params['latent_flag'])
	reciprocal_flag = copy.deepcopy(params['reciprocal_flag'])
	fintrinsic_base = copy.deepcopy(params['fintrinsic_base'])
	tcdata_std = copy.deepcopy(params['tcdata_std'])
	std_scale = copy.deepcopy(params['std_scale'])

	# DBname = copy.deepcopy(params['DBname'])
	# DBnum = copy.deepcopy(params['DBnum'])

	# load the network model again in case it has been changed for DCM
	network, nclusterlist, sem_region_list, fintrinsic_count, vintrinsic_count, fintrinsic_base = pysapm.load_network_model_w_intrinsics(networkmodel_DCM)

	# setup the cluster numbers and load the data for the chosen clusters
	cnum_base = np.array([np.sum(nclusterlist[:x]) for x in range(len(nclusterlist))]).astype(int)
	clusterlist = cnum_base + np.array(cnums)

	NP = len(nruns_per_person)
	nn = 0   # each person
	tsize_full = tsize * nruns_per_person[nn]

	tp = copy.deepcopy(tplist_full[0][nn]['tp'])
	Sinput = []
	for nc, cval in enumerate(clusterlist):
		tc1 = tcdata_centered[cval, tp]
		Sinput.append(tc1)
	Sin_BOLD = np.array(Sinput)

	# define the latent inputs for the DCM network model
	if epoch >= tsize:
		et1 = 0
		et2 = tsize
	else:
		if np.floor(epoch / 2).astype(int) == np.ceil(epoch / 2).astype(int):  # even numbered epoch
			et1 = (timepoint - np.floor(epoch / 2)).astype(int)
			et2 = (timepoint + np.floor(epoch / 2)).astype(int)
		else:
			et1 = (timepoint - np.floor(epoch / 2)).astype(int) - 1
			et2 = (timepoint + np.floor(epoch / 2)).astype(int)
	if et1 < 0: et1 = 0
	if et2 > tsize: et2 = tsize
	epoch = et2 - et1

	ftemp = fintrinsic_base[0, et1:et2]
	fintrinsic1 = np.array(list(ftemp) * nruns_per_person[nn])[:,np.newaxis]

	Mintrinsic = copy.deepcopy(fintrinsic1)

	DCMresults = test_DCM_performance(network, tsize_full, nruns_per_person[nn],
									  Mconn, Minput, Sin_BOLD.T, beta_list, ctarget, csource,
									  dtarget, dsource, Mintrinsic.T, latent_flag,
									  reciprocal_flag)



